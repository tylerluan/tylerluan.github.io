

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.jpg">
  <link rel="icon" href="/img/favicon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="ykk648">
  <meta name="keywords" content="">
  
    <meta name="description" content="2021年论文阅读笔记合集">
<meta property="og:type" content="article">
<meta property="og:title" content="阅读论文笔记合集-2021">
<meta property="og:url" content="http://example.com/posts/65074/index.html">
<meta property="og:site_name" content="ykk648&#39;s hub">
<meta property="og:description" content="2021年论文阅读笔记合集">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/20211231-JonesBeachHarpSeal_ZH-CN9584238333_UHD.jpg">
<meta property="article:published_time" content="2021-12-27T10:24:57.000Z">
<meta property="article:modified_time" content="2022-01-19T09:53:49.000Z">
<meta property="article:author" content="ykk648">
<meta property="article:tag" content="Essays">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/20211231-JonesBeachHarpSeal_ZH-CN9584238333_UHD.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>阅读论文笔记合集-2021 - ykk648&#39;s hub</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ykk648&#39;s hub</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/20211231-JonesBeachHarpSeal_ZH-CN9584238333_UHD.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="阅读论文笔记合集-2021"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2021-12-27 18:24" pubdate>
          2021年12月27日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.9k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          66 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">阅读论文笔记合集-2021</h1>
            
            
              <div class="markdown-body">
                
                <p>2021年论文阅读笔记合集</p>
<span id="more"></span>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/TracelessLe/article/details/106593425">normalization</a></p>
<h3 id="Face-Swap"><a href="#Face-Swap" class="headerlink" title="Face Swap"></a>Face Swap</h3><h4 id="FSGAN"><a href="#FSGAN" class="headerlink" title="FSGAN"></a>FSGAN</h4><p>paper：</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.05932.pdf">https://arxiv.org/pdf/1908.05932.pdf</a></p>
<p>官方repo：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/YuvalNirkin/fsgan"> https://github.com/YuvalNirkin/fsgan</a></p>
<hr>
<h4 id="Disney高清换脸"><a href="#Disney高清换脸" class="headerlink" title="Disney高清换脸"></a>Disney高清换脸</h4><p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/%E7%81%AB%E7%8B%90%E6%88%AA%E5%9B%BE_2022-01-05T11-27-25.197Z.png" srcset="/img/loading.gif" lazyload></p>
<hr>
<h4 id="InfoSwap"><a href="#InfoSwap" class="headerlink" title="InfoSwap"></a>InfoSwap</h4><p>Information Bottleneck Disentanglement for Identity Swapping</p>
<p>paper: <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_Information_Bottleneck_Disentanglement_for_Identity_Swapping_CVPR_2021_paper.pdf">https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_Information_Bottleneck_Disentanglement_for_Identity_Swapping_CVPR_2021_paper.pdf</a></p>
<p>codes: <a target="_blank" rel="noopener" href="https://github.com/GGGHSL/InfoSwap-master">https://github.com/GGGHSL/InfoSwap-master</a></p>
<p>两点创新，一是 将<a target="_blank" rel="noopener" href="http://192.168.6.167:8090/display/CV/IBA">IBA</a>引入到arcface网络中：</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image2021-12-20_11-19-10.png" srcset="/img/loading.gif" lazyload></p>
<p>第二个是在计算IBA提取的feature loss的时候，不仅让output src cos距离最小，还让output target cos距离变大</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220105192351796.png" srcset="/img/loading.gif" lazyload></p>
<hr>
<h4 id="ShapeEditer"><a href="#ShapeEditer" class="headerlink" title="ShapeEditer"></a>ShapeEditer</h4><p>基于stylegan2，arcface提取id特征，pspnet提取attr特征，映射到latent，使用stylegan2生成图片</p>
<p>优点： 生成图片的分辨率达到1024*1024</p>
<p>缺点：不像<img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220110175953700.png" srcset="/img/loading.gif" lazyload></p>
<hr>
<h4 id="FaceShifter"><a href="#FaceShifter" class="headerlink" title="FaceShifter"></a>FaceShifter</h4><p>SPatially-Adaptive (DE) normalization（SPADE）</p>
<p>adaptive embedding integration networ &#x2F; AEI</p>
<p>adaptive attentional denormalization generator &#x2F; AAD</p>
<p>参考AdaIN SPADE 利用denormalizion 在multi feature layer 做feature integration<img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220110175646046.png" srcset="/img/loading.gif" lazyload></p>
<p>loss:</p>
<ul>
<li>reconstruction loss</li>
<li>adversarial loss</li>
<li>attribute loss  (multi level) </li>
<li>identity loss</li>
</ul>
<hr>
<h4 id="HifiFace"><a href="#HifiFace" class="headerlink" title="HifiFace"></a>HifiFace</h4><p>复现效果可以，3D特征使用deep3dfacerecon，人脸特征使用curricularface, mask使用HRNet</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220110165703904.png" srcset="/img/loading.gif" lazyload></p>
<p>预处理上，训练集VGGFace2，使用人脸特征+3D特征，其中3D特征使用其中的表情和动作，人脸特征使用</p>
<p>训练中，分为低分辨率和高分辨率两个分支，低分辨率使用1&#x2F;4 feature size，方便身份和属性解耦，</p>
<p>同时SFF（semantic facial fusion module）会预测mask，解决遮挡和面部边缘融合问题</p>
<p>loss上，shape loss &#x2F; id loss &#x2F; segmentation loss &#x2F; reconstruction loss &#x2F; cycle loss &#x2F; lpips loss</p>
<hr>
<h3 id="Super-Resolution"><a href="#Super-Resolution" class="headerlink" title="Super Resolution"></a>Super Resolution</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/50757421">PSNR SSIM </a>   <a target="_blank" rel="noopener" href="https://www.cnblogs.com/wxl845235800/p/7692788.html">ringing artifacts&#x2F;sinc&#x2F; </a>   <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/63957812">spectral normalization </a> <a target="_blank" rel="noopener" href="https://blog.csdn.net/u013043762/article/details/115908872">FID</a></p>
<hr>
<h4 id="ESRGAN-amp-Real-ESRGAN"><a href="#ESRGAN-amp-Real-ESRGAN" class="headerlink" title="ESRGAN &amp; Real-ESRGAN"></a>ESRGAN &amp; Real-ESRGAN</h4><p>   <strong>ESRGAN</strong> 整体基于SRGAN改进</p>
<ol>
<li><p><strong>RDDB</strong>结构，去掉residual block中的BN，使用dense block结构增加大量连接</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220106155628246.png" srcset="/img/loading.gif" lazyload></p>
</li>
<li><p><strong>Relativistic Discriminator</strong> 相对鉴别器</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220106160004355.png" srcset="/img/loading.gif" lazyload></p>
</li>
<li><p><strong>Perceptual Loss</strong> 感知损失，移到激活前，解决激活后特征稀疏导致的亮度问题</p>
</li>
<li><p><strong>Network Interpolation</strong> 网络插值，先训练一个基于PSNR的模型，然后在GAN上finetune，二者结合</p>
</li>
</ol>
<hr>
<p><strong>Real-ESRGAN</strong> 基于ESRGAN</p>
<ol>
<li><p>通过2-stage的image degradation来构建训练集</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220106165326026.png" srcset="/img/loading.gif" lazyload></p>
</li>
<li><p>通过在first order和second order中加入sinc filter来模拟ringing artifact&#x2F;overshoot 现象</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220106165446004.png" srcset="/img/loading.gif" lazyload></p>
</li>
<li><p>pixel unshuffle操作，来解决1x 2x的放大问题</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220106170120869.png" srcset="/img/loading.gif" lazyload></p>
</li>
<li><p>鉴别器替换为U-Net，加入spectral normalization ，使鉴别器满足1-Lipschitz条件，使生成器梯度不会消失</p>
</li>
</ol>
<hr>
<h4 id="GFPGAN"><a href="#GFPGAN" class="headerlink" title="GFPGAN"></a>GFPGAN</h4><p>出自腾讯，复现效果较真实</p>
<ol>
<li>使用U-net构建一个degration removal模块，连接预训练StyleGAN2模型，分别使用latent和channal split SFT（Spatial Feature Transform）连接。<img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220107112159180.png" srcset="/img/loading.gif" lazyload></li>
<li>训练数据使用FFHQ做退化生成，测试集使用合成开源数据（应该加入了私有数据)</li>
<li>三个loss如图，加强了五官loss，增加identity loss</li>
</ol>
<hr>
<h4 id="GPEN"><a href="#GPEN" class="headerlink" title="GPEN"></a>GPEN</h4><p>出自阿里，定义BFR（blind face restoration）问题，复现效果色彩略微失真</p>
<p>先训练一个能生成高清人脸的GAN（FFHQ），然后把这个预训练的GAN嵌入到一个UNet结构中，作为它的解码器部分，这之后，整个网络再通过人工降质的数据对来进行微调训练。</p>
<p>在质量评估中使用了PSNR FID LPIPS</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220107112816853.png" srcset="/img/loading.gif" lazyload></p>
<hr>
<h4 id="DFDNet"><a href="#DFDNet" class="headerlink" title="DFDNet"></a>DFDNet</h4><p>类似查找匹配，先用大量数据训练多尺度的component字典，然后使用这些字典训练DFT结构，中间提出CAdaIN（component AdaIN）.</p>
<p>实测效果较好，但是生成痕迹明显，同时由于需要对五官做align，速度非常慢。</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220107114403024.png" srcset="/img/loading.gif" lazyload></p>
<hr>
<h3 id="ViT-Transformer"><a href="#ViT-Transformer" class="headerlink" title="ViT Transformer"></a>ViT Transformer</h3><h4 id="ViT"><a href="#ViT" class="headerlink" title="ViT"></a>ViT</h4><p>google brain.</p>
<p>ViT的基本思想： 将图片划分为固定尺寸patch，拉平后增加position embedding（代表位置信息），经过transfomer encoder和MLP head（Transfomer结构），</p>
<p>注意额外使用一个class token（learnable embedding ）表示分类结果，避免对某个patch的偏重</p>
<p>优势： transfomer的跨模态能力，GPU友好，适合大规模计算，计算力上限高于CNN</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220107151841306.png" srcset="/img/loading.gif" lazyload></p>
<hr>
<h4 id="Token-Labeling"><a href="#Token-Labeling" class="headerlink" title="Token Labeling"></a>Token Labeling</h4><p>起名LV-ViT，基于ViT，除了class对class token的监督以外，还引入了一个额外的score map对每个token进行监督，这个额外的source map来自另一个训练好的网络。</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220107152029775.png" srcset="/img/loading.gif" lazyload></p>
<hr>
<h4 id="Tokens-to-Token-ViT"><a href="#Tokens-to-Token-ViT" class="headerlink" title="Tokens-to-Token ViT"></a>Tokens-to-Token ViT</h4><p>来自依图</p>
<p>基于ViT, 解决了ViT token长度一样和计算量大的问题， 思路是在ViT前面增加T2T结构，通过soft split + Unfold操作渐进式减少token的长度：</p>
<p>soft split: 在将图片切分成patch的时候，进行overlap，建立一个先验，即距离近的patch更相关</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220107152147049.png" srcset="/img/loading.gif" lazyload></p>
<p>和ViT相比计算量下降明显，在<a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/image-classification-on-imagenet">Imagenet</a>上面仍然能达到SOTA</p>
<p>在T2T结构的选择上，对比了五种结构，最终选择使用deep narrow的结构：</p>
<ol>
<li><p>Dense connection as DenseNet [21];</p>
</li>
<li><p>Deep-narrow vs. shallow-wide structure as in Wide-ResNets [52];</p>
</li>
<li><p>Channel attention as Squeeze-an-Excitation (SE) Net-works [20];</p>
</li>
<li><p>More split heads in multi-head attention layer asResNeXt [44];</p>
</li>
<li><p>Ghost operations as GhostNet [14]</p>
</li>
</ol>
<p>此外，T2T可以使用Performer代替Transformer以降低GPU占用 （Rethinking transformer based set prediction for object detection）</p>
<hr>
<h4 id="Attention-is-all-you-need"><a href="#Attention-is-all-you-need" class="headerlink" title="Attention is all you need"></a>Attention is all you need</h4><p>用于机器翻译，保持encoder-decoder结构，完全抛弃rnn和cnn，使用transformer架构</p>
<p>提出self-attention，包括query key value，由三个不同的权值矩阵得到</p>
<p>提出multi head attention，等于多个self-attention的ensemble</p>
<p>引入位置编码(position embedding)</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220111095625685.png" srcset="/img/loading.gif" lazyload></p>
<hr>
<h3 id="3D-Face-Reconstruction"><a href="#3D-Face-Reconstruction" class="headerlink" title="3D Face Reconstruction"></a>3D Face Reconstruction</h3><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/d2fb5b8147ca">3DMM</a>(3D morphable models )三维可变形人脸模型，每张脸表现为texture和shape的叠加，</p>
<p>基于传统方法model fit构建数据训练3DMM CNN，</p>
<h4 id="3DDFA-amp-3DDFA-V2"><a href="#3DDFA-amp-3DDFA-V2" class="headerlink" title="3DDFA &amp; 3DDFA_V2"></a>3DDFA &amp; 3DDFA_V2</h4><p>3DDFA(3D Dense Face Alignment) 使用PNNC(Projected Normalized Coordinate Code) 作为特征预测，而PRNet使用UV</p>
<p>提出PAC(pose adaptive convolution),在语义一致性位置进行卷积</p>
<p>loss上分为参数距离（parameter distance cost）和顶点距离（vertex distance cost）,一个最小化参数误差，一个最小化顶点距离，提高3DMM形变能力</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220111111909095.png" srcset="/img/loading.gif" lazyload></p>
<p>此外提出对训练数据大角度的增强，构建大姿态人脸库来训练模型</p>
<p>3DDFA_V2，针对3DDFA进行了提速，主要包括三点：</p>
<ul>
<li>landmark-regression regularization 加速拟合</li>
<li>mata-joint optimization 动态组合loss，加速拟合</li>
<li>3d aided short-video-synthesis 使用静止图片生成短视频训练数据，提高video上的对齐效果</li>
</ul>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220111113843858.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="DECA"><a href="#DECA" class="headerlink" title="DECA"></a>DECA</h4><p><a target="_blank" rel="noopener" href="https://flame.is.tue.mpg.de/">FLAME</a></p>
<p>DECA模型分为粗糙的人脸重建Ec（resnet50构成）,精细的人脸重建Ed。具体的Ec包括了相机c，反射率ɑ，光照l，形状β，姿态θ，表情Ψ。Ed包括了细节δ。</p>
<p>粗糙模型的损失包括了人脸关键点的损失 Llmk，眼睛闭合的损失 Leye，基于照片的损失 Lpho，ID损失 Lid，形状连续损失 Lsc，正则化损失 Lreg。</p>
<p>精细的人脸重建损失，包含了照片细节损失 LphoD， ID-MRF损失 Lmrf，软对称性损失 Lsym，细节正则化损失 LregD。</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220111140747470.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="Deep3DFaceRecon"><a href="#Deep3DFaceRecon" class="headerlink" title="Deep3DFaceRecon"></a>Deep3DFaceRecon</h4><p>对R-Net提取的人脸相关系数进行重建，加入identity loss（arcface），引入光度损失（robust photometric loss）和关键点损失（landmark location loss），</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220111141426885.png" srcset="/img/loading.gif" lazyload></p>
<hr>
<h4 id="Detailed3DFace"><a href="#Detailed3DFace" class="headerlink" title="Detailed3DFace"></a>Detailed3DFace</h4><p>首先提供一个<a target="_blank" rel="noopener" href="https://facescape.nju.edu.cn/Page_Data/">数据集</a></p>
<p>包含120G , TU models（938ID*20expression）和bilinear model</p>
<p>一个生成detail 3D face的demo,没有开源landmark检测器，代码中使用dlib替代</p>
<p>预测置换贴图（displacement map）使用pix2pixHD结构，使用dynamic details synthesis结合19个关键表情预测细节</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220111143916975.png" srcset="/img/loading.gif" lazyload></p>
<p>使用<a target="_blank" rel="noopener" href="https://github.com/yanght321/Detailed3DFace">Detailed3DFace</a> 可以获得：</p>
<p>一份半脸obj，一张UV贴图，一张对应的displacement map，20张其他表情的displacement map，和简单重建结果</p>
<p>使用zbrush重建结果：</p>
<img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220111142851054.png" srcset="/img/loading.gif" lazyload style="max-width: 30%;" />

<hr>
<h4 id="帝国理工组"><a href="#帝国理工组" class="headerlink" title="帝国理工组"></a>帝国理工组</h4><p><strong>基础</strong></p>
<p>3D模态（3D modalities） 包括 texture shape normals(法线) expression</p>
<p>帝国理工的<strong>LSFM</strong> <a target="_blank" rel="noopener" href="https://github.com/menpo/lsfm">https://github.com/menpo/lsfm</a> ， 中提到了<strong>MeIn3D</strong>数据集，包含10000张人脸，数据无法下载，参考<a target="_blank" rel="noopener" href="https://github.com/menpo/lsfm/issues/27">https://github.com/menpo/lsfm/issues/27</a></p>
<p><strong>CFHM</strong> Combining 3d morphable models: A large scale face-and-head model 2019  一种人脸和人头大规模模型</p>
<p><strong>MICC</strong> 53个人头数据 <a target="_blank" rel="noopener" href="http://www.micc.unifi.it/vim/3dfaces-dataset/index.html#!prettyPhoto">http://www.micc.unifi.it/vim/3dfaces-dataset/index.html#!prettyPhoto</a></p>
<h5 id="TBGAN"><a href="#TBGAN" class="headerlink" title="TBGAN"></a>TBGAN</h5><p>创新点： 提出一种trunk-branch architecture(主干分支结构)，生成将texture和geometry耦合在一起的人脸</p>
<p>使用MeIn3D数据集， 将shape texture normals所有数据在UV平面对齐，以便输入GAN网络，增加一个表情识别网络，以操纵生成人脸的表情，基于CFHM将面部映射到头部</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220112143606102.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="GANFit-amp-Fast-GANFit"><a href="#GANFit-amp-Fast-GANFit" class="headerlink" title="GANFit &amp; Fast-GANFit"></a>GANFit &amp; Fast-GANFit</h5><p>已经商业化，不会开源</p>
<p>其中ps和pe与3DMM一致，pt是由人脸latent提取UV map的网络，也就是shape使用3dmm，texture使用GAN生成，</p>
<p>加入一些loss保证GAN的生成质量，包括identity loss &#x2F; content loss &#x2F; pixel loss &#x2F; landmark loss</p>
<p>最终结果使用MICC数据集评价</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220112144643819.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="OSTeC"><a href="#OSTeC" class="headerlink" title="OSTeC"></a>OSTeC</h5><p>为了解决3d数据难以采集的问题，用2D生成器生成多角度的2D图片，然后构建3D数据集，用于训练3DMM</p>
<p>模型开源，需要申请</p>
<p>图片生成使用stylegan2，loss包括photometric loss(光度损失)，identity loss, perceptual loss(感知损失), landmark loss， 渐进式生成纹理，最终stitching成一个UV map</p>
<p>能够达到GANFit的水平（10000张真实3d数据）：</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220112144738247.png" srcset="/img/loading.gif" lazyload></p>
<h5 id="AvatarMe-amp-AvatarMe"><a href="#AvatarMe-amp-AvatarMe" class="headerlink" title="AvatarMe &amp; AvatarMe++"></a>AvatarMe &amp; AvatarMe++</h5><p>用于实现 accurate facial skin diffuse and specular reflection,  self-occlusion and subsurface scattering approximation  （准确的面部皮肤漫反射和镜面反射、自遮挡和次表面散射近似）</p>
<p>先使用3dmm推理图片获得shape texture normals，然后进行超分获得高分辨率uv map，使用网络转换为4类漫反射 镜面反射 等贴图，进行光源渲染，然后计算loss</p>
<p>结果可以很好的使用任意图片进行3d重建，同时生成不同的光源渲染：</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220112144827024.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="3D-Human-Digitization"><a href="#3D-Human-Digitization" class="headerlink" title="3D Human Digitization"></a>3D Human Digitization</h3><p><a target="_blank" rel="noopener" href="https://github.com/digital-standard/ThreeDPoseTracker">TDPT</a></p>
<h4 id="Contact-Human-Dynamics"><a href="#Contact-Human-Dynamics" class="headerlink" title="Contact Human Dynamics"></a>Contact Human Dynamics</h4><p>基于openpose和MTC（Monocular Total Capture），分别提2D和3D pose，使用运动估计，对姿态和脚贴地有更好的支持。</p>
<p>其中运动学优化使用<a target="_blank" rel="noopener" href="https://github.com/ethz-adrl/towr">Towr</a></p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220119150733562.png" srcset="/img/loading.gif" lazyload></p>
<hr>
<h4 id="HuMoR"><a href="#HuMoR" class="headerlink" title="HuMoR"></a>HuMoR</h4><p>与contact human dynamics师出同门，第一篇contact human是用towr做运动学优化，第二篇humor提出了CVAE（conditional variational autoencoder）方法，是用编解码器去预测前后帧的运动学参数。</p>
<p>humor优化分 3 个阶段进行，第 1 和第 2 阶段是使用姿势先验和平滑（即 <a target="_blank" rel="noopener" href="https://github.com/nghorbani/human_body_prior">VPoser-t 基线</a>）进行初始化，第 3 阶段是使用 HuMoR 运动先验进行全面优化。</p>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220119151139829.png" srcset="/img/loading.gif" lazyload></p>
<hr>
<h4 id="DeepMotion"><a href="#DeepMotion" class="headerlink" title="DeepMotion"></a>DeepMotion</h4><p>阅读deepmotion的API文档，发现提到<strong>poseEstimation.footLockingMode</strong>，很显然是对脚部锁定增加了额外选项</p>
<p><a target="_blank" rel="noopener" href="https://github.com/DeepMotion/Animate-3D-REST-API">https://github.com/DeepMotion/Animate-3D-REST-API</a></p>
<p>参考deepmotion官网，deepmotion的运动学计算主要由motion brain完成，在英特尔宣传片中提到motion brain使用英特尔CPU加速技术进行优化，应该是运动学优化，和GPU无关</p>
<p><a target="_blank" rel="noopener" href="https://www.deepmotion.com/ai-motion-brain">https://www.deepmotion.com/ai-motion-brain</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=3acpP_WfqJY&amp;t=32s">https://www.youtube.com/watch?v=3acpP_WfqJY&amp;t=32s</a></p>
<p>在deepmotion演示视频中看到pose estimation一样非常抖动</p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Edia1conVAY">https://www.youtube.com/watch?v=Edia1conVAY</a></p>
<p>deepmotion也在做移动端适配工作，提到三步骤：constructs 3D human motion + physically simulates the result + retargets the  motion to a desired character rig</p>
<p>Optimized for both speed and  footprint on PC and mobile, our industry leading Body Tracking &amp;  Interaction SDK constructs 3D human motion, physically simulates the  result, and retargets the motion to a desired character rig - all at  run-time. Available for Windows, Linux, and mobile platforms.</p>
<p><a target="_blank" rel="noopener" href="https://www.deepmotion.com/3d-body-tracking">https://www.deepmotion.com/3d-body-tracking</a></p>
<p>查阅发现UE4本身在17年就出现了foot locking插件</p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=uqsXmH4KBls">https://www.youtube.com/watch?v=uqsXmH4KBls</a></p>
<p><a target="_blank" rel="noopener" href="https://www.unrealengine.com/marketplace/en-US/product/advanced-locomotion-system-v1">https://www.unrealengine.com/marketplace/en-US/product/advanced-locomotion-system-v1</a></p>
<p>使用locomotion做动画的实践</p>
<p>[<a target="_blank" rel="noopener" href="https://thousun.github.io/2021/07/24/No.%20002%20Advanced%20locomotion%E6%8B%86%E8%A7%A3%E2%80%94P1.ABP/]">https://thousun.github.io/2021/07/24/No.%20002%20Advanced%20locomotion%E6%8B%86%E8%A7%A3%E2%80%94P1.ABP/]</a>(<a target="_blank" rel="noopener" href="https://thousun.github.io/2021/07/24/No">https://thousun.github.io/2021/07/24/No</a>. 002 Advanced locomotion拆解—P1.ABP&#x2F;)</p>
<p>查看最新版本的animate 3D更新日志：</p>
<ul>
<li><strong>Improved Ground Motions:</strong> Animation quality will see an improvement for ground motions that involve weight on hands, feet and other joints.</li>
<li><strong>Improved Ground Contact:</strong> General overall improved animation quality for limb ground contact and anti-tilting.</li>
<li><strong>Improved Tracking of Slower Movements:</strong> Real-time motions that are slower will see improved tracking.</li>
</ul>
<p>通过对手 脚 关节重量的调整，优化了接地动作</p>
<p>提升身体各部分的接地质量</p>
<p><a target="_blank" rel="noopener" href="https://blog.deepmotion.com/2021/10/22/v34/">https://blog.deepmotion.com/2021/10/22/v34/</a></p>
<p>猜测deepmotion的离线处理主要由pose estimation + motion brain，motion brain的运动学优化与locomotion类似，需要大量cpu算力，并且无法做到实时</p>
<p>不排除在这个运动学优化的过程中，deepmotion增加了深度学习替代某些环节</p>
<p>最新推出的移动端应该进行了优化或者阉割，或者将某些计算步骤上云</p>
<hr>
<h4 id="PIFuHD-amp-MonoPort-amp-NeuralBody"><a href="#PIFuHD-amp-MonoPort-amp-NeuralBody" class="headerlink" title="PIFuHD &amp; MonoPort &amp; NeuralBody"></a>PIFuHD &amp; MonoPort &amp; NeuralBody</h4><p>思路很好，但one-shot做人体3D重建，效果太差了。</p>
<ul>
<li>之前的pifu最大输入尺寸限制到512<em>512，pifuhd将输入尺寸提高到1024</em>1024</li>
<li>分为两个level，一个level是1024降采样到512，提取128分辨率的特征；另一个fine level，输入1024，提取512分辨率的特征</li>
</ul>
<p><img src="https://blog2021-1252404748.cos.ap-nanjing.myqcloud.com/image-20220119152835967.png" srcset="/img/loading.gif" lazyload></p>
<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Notes/" class="category-chain-item">Notes</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Essays/">#Essays</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>阅读论文笔记合集-2021</div>
      <div>http://example.com/posts/65074/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>ykk648</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2021年12月27日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/posts/65394/" title="阅读论文笔记合集-2022">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">阅读论文笔记合集-2022</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/posts/42935/" title="confluence安装记录">
                        <span class="hidden-mobile">confluence安装记录</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"lzlf3QD0Q6UHueruqIPwFC72-gzGzoHsz","appKey":"VdSg53GY7DQXNimQOtiPFxOo","path":"window.location.pathname","placeholder":"说点什么","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"appid":"lzlf3QD0Q6UHueruqIPwFC72-gzGzoHsz","appkey":"VdSg53GY7DQXNimQOtiPFxOo"},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/ykk648" target="_blank" rel="nofollow noopener"><span>ykk648</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
